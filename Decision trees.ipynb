{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"In a decision tree, we decide if a node will be split or not by looking at the **information gain** that split would give us. (Image of video IG)\n\nWhere \n\n$$\\text{Information Gain} = H(p_1^\\text{node})- \\left(w^{\\text{left}}H\\left(p_1^\\text{left}\\right) + w^{\\text{right}}H\\left(p_1^\\text{right}\\right)\\right),$$\n\nand $H$ is the entropy, defined as\n\n$$H(p_1) = -p_1 \\text{log}_2(p_1) - (1- p_1) \\text{log}_2(1- p_1)$$\n\nRemember that log here is defined to be in base 2. Run the code block below to see by yourself how the entropy. $H(p)$ behaves while $p$ varies.\n\nNote that the H attains its higher value when $p = 0.5$. This means that the probability of event is $0.5$. And its minimum value is attained in $p = 0$ and $p = 1$, i.e., the probability of the event happening is totally predictable. Thus, the entropy shows the degree of predictability of an event.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-15T15:55:46.641591Z","iopub.execute_input":"2025-08-15T15:55:46.642183Z","iopub.status.idle":"2025-08-15T15:55:46.646261Z","shell.execute_reply.started":"2025-08-15T15:55:46.642153Z","shell.execute_reply":"2025-08-15T15:55:46.645398Z"}},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":"We will use **one-hot encoding** to encode the categorical features. They will be as follows:\n\n- Ear Shape: Pointy = 1, Floppy = 0\n- Face Shape: Round = 1, Not Round = 0\n- Whiskers: Present = 1, Absent = 0\n\nTherefore, we have two sets:\n\n- `X_train`: for each example, contains 3 features:\n            - Ear Shape (1 if pointy, 0 otherwise)\n            - Face Shape (1 if round, 0 otherwise)\n            - Whiskers (1 if present, 0 otherwise)\n            \n- `y_train`: whether the animal is a cat\n            - 1 if the animal is a cat\n            - 0 otherwise","metadata":{}},{"cell_type":"code","source":"X_train = np.array([[1, 1, 1],\n[0, 0, 1],\n [0, 1, 0],\n [1, 0, 1],\n [1, 1, 1],\n [1, 1, 0],\n [0, 0, 0],\n [1, 1, 0],\n [0, 1, 0],\n [0, 1, 0]])\n\ny_train = np.array([1, 1, 0, 0, 1, 1, 0, 1, 0, 0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T15:55:46.647605Z","iopub.execute_input":"2025-08-15T15:55:46.647947Z","iopub.status.idle":"2025-08-15T15:55:46.668884Z","shell.execute_reply.started":"2025-08-15T15:55:46.647916Z","shell.execute_reply":"2025-08-15T15:55:46.668031Z"}},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":"This means that the first example has a pointy ear shape, round face shape and it has whiskers.","metadata":{}},{"cell_type":"markdown","source":"On each node, we compute the information gain for each feature, then split the node on the feature with the higher information gain, by comparing the entropy of the node with the weighted entropy in the two splitted nodes. \n","metadata":{}},{"cell_type":"markdown","source":"So, the root node has every animal in our dataset. Remember that $p_1^{node}$ is the proportion of positive class (cats) in the root node. So\n\n$$p_1^{node} = \\frac{5}{10} = 0.5$$\n\nNow let's write a function to compute the entropy.","metadata":{}},{"cell_type":"code","source":"def entropy(p):\n    if p == 0 or p == 1:\n        return 0\n    else:\n        return -p * np.log2(p) - (1- p)*np.log2(1 - p)\n    \nprint(entropy(0.5))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T15:55:46.670150Z","iopub.execute_input":"2025-08-15T15:55:46.670428Z","iopub.status.idle":"2025-08-15T15:55:46.686723Z","shell.execute_reply.started":"2025-08-15T15:55:46.670406Z","shell.execute_reply":"2025-08-15T15:55:46.685889Z"}},"outputs":[{"name":"stdout","text":"1.0\n","output_type":"stream"}],"execution_count":21},{"cell_type":"markdown","source":"To illustrate, let's compute the information gain if we split the node for each of the features. To do this, let's write some functions.","metadata":{}},{"cell_type":"code","source":"def split_indices(X, index_feature):\n    left_indices = []\n    right_indices = []\n    for i,x in enumerate(X):\n        if x[index_feature] == 1:\n            left_indices.append(i)\n        else:\n            right_indices.append(i)\n    return left_indices, right_indices","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T15:55:46.687781Z","iopub.execute_input":"2025-08-15T15:55:46.688696Z","iopub.status.idle":"2025-08-15T15:55:46.704286Z","shell.execute_reply.started":"2025-08-15T15:55:46.688675Z","shell.execute_reply":"2025-08-15T15:55:46.703271Z"}},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":"So, if we choose Ear Shape to split, then we must have in the left node (check the table above) the indices:\n\n$$0 \\quad 3 \\quad 4 \\quad 5 \\quad 7$$\n\nand the right indices, the remaining ones.","metadata":{}},{"cell_type":"code","source":"split_indices(X_train, 0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T15:55:46.706012Z","iopub.execute_input":"2025-08-15T15:55:46.706296Z","iopub.status.idle":"2025-08-15T15:55:46.725163Z","shell.execute_reply.started":"2025-08-15T15:55:46.706270Z","shell.execute_reply":"2025-08-15T15:55:46.724303Z"}},"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"([0, 3, 4, 5, 7], [1, 2, 6, 8, 9])"},"metadata":{}}],"execution_count":23},{"cell_type":"markdown","source":"Now we need another function to compute the weighted entropy in the splitted nodes. We must find:\n\n- $w^{\\text{left}}$ and $w^{\\text{right}}$, the proportion of animals in **each node**.\n- $p^{\\text{left}}$ and $p^{\\text{right}}$, the proportion of cats in **each split**.\n\nNote the difference between these two definitions!! To illustrate, if we split the root node on the feature of index 0 (Ear Shape), then in the left node, the one that has the animals 0, 3, 4, 5 and 7, we have:\n\n$$w^{\\text{left}}= \\frac{5}{10} = 0.5 \\text{ and } p^{\\text{left}} = \\frac{4}{5}$$\n$$w^{\\text{right}}= \\frac{5}{10} = 0.5 \\text{ and } p^{\\text{right}} = \\frac{1}{5}$$","metadata":{}},{"cell_type":"code","source":"def weighted_entropy(X,y,left_indices,right_indices):\n    w_left = len(left_indices)/len(X)\n    w_right = len(right_indices)/len(X)\n    p_left = sum(y[left_indices])/len(left_indices)\n    p_right = sum(y[right_indices])/len(right_indices)\n    \n    weighted_entropy = w_left * entropy(p_left) + w_right * entropy(p_right)\n    return weighted_entropy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T15:55:46.726054Z","iopub.execute_input":"2025-08-15T15:55:46.726356Z","iopub.status.idle":"2025-08-15T15:55:46.741827Z","shell.execute_reply.started":"2025-08-15T15:55:46.726335Z","shell.execute_reply":"2025-08-15T15:55:46.740955Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"left_indices, right_indices = split_indices(X_train, 0)\nweighted_entropy(X_train, y_train, left_indices, right_indices)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T15:55:46.742780Z","iopub.execute_input":"2025-08-15T15:55:46.743470Z","iopub.status.idle":"2025-08-15T15:55:46.763119Z","shell.execute_reply.started":"2025-08-15T15:55:46.743448Z","shell.execute_reply":"2025-08-15T15:55:46.762237Z"}},"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"0.7219280948873623"},"metadata":{}}],"execution_count":25},{"cell_type":"markdown","source":"So, the weighted entropy in the 2 split nodes is 0.72. To compute the **Information Gain** we must subtract it from the entropy in the node we chose to split (in this case, the root node). ","metadata":{}},{"cell_type":"code","source":"def information_gain(X, y, left_indices, right_indices):\n    p_node = sum(y)/len(y)\n    h_node = entropy(p_node)\n    w_entropy = weighted_entropy(X,y,left_indices,right_indices)\n    return h_node - w_entropy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T15:55:46.764053Z","iopub.execute_input":"2025-08-15T15:55:46.764389Z","iopub.status.idle":"2025-08-15T15:55:46.783254Z","shell.execute_reply.started":"2025-08-15T15:55:46.764336Z","shell.execute_reply":"2025-08-15T15:55:46.782190Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"information_gain(X_train, y_train, left_indices, right_indices)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T15:55:46.784162Z","iopub.execute_input":"2025-08-15T15:55:46.784466Z","iopub.status.idle":"2025-08-15T15:55:46.803060Z","shell.execute_reply.started":"2025-08-15T15:55:46.784438Z","shell.execute_reply":"2025-08-15T15:55:46.802173Z"}},"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"0.2780719051126377"},"metadata":{}}],"execution_count":27},{"cell_type":"markdown","source":"Now, let's compute the information gain if we split the root node for each feature:","metadata":{}},{"cell_type":"code","source":"for i, feature_name in enumerate(['Ear Shape', 'Face Shape', 'Whiskers']):\n    left_indices, right_indices = split_indices(X_train, i)\n    i_gain = information_gain(X_train, y_train, left_indices, right_indices)\n    print(f\"Feature: {feature_name}, information gain if we split the root node using this feature: {i_gain:.2f}\")\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T15:55:46.804202Z","iopub.execute_input":"2025-08-15T15:55:46.804536Z","iopub.status.idle":"2025-08-15T15:55:46.821072Z","shell.execute_reply.started":"2025-08-15T15:55:46.804515Z","shell.execute_reply":"2025-08-15T15:55:46.820248Z"}},"outputs":[{"name":"stdout","text":"Feature: Ear Shape, information gain if we split the root node using this feature: 0.28\nFeature: Face Shape, information gain if we split the root node using this feature: 0.03\nFeature: Whiskers, information gain if we split the root node using this feature: 0.12\n","output_type":"stream"}],"execution_count":28},{"cell_type":"markdown","source":"So, the best feature to split is indeed the Ear Shape. Run the code below to see the split in action. You do not need to understand the following code block. ","metadata":{}}]}